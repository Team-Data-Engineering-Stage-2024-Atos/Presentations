{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4258a01b-8054-44ac-b871-4b49fbf1fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Sample data for illustration purposes\n",
    "owid_csv = \"\"\"date,location,new_cases,new_deaths,total_cases,total_deaths,reproduction_rate,icu_patients,hosp_patients,weekly_icu_admissions,weekly_hosp_admissions,total_tests,new_tests,total_vaccinations,people_vaccinated,people_fully_vaccinated,total_boosters,new_vaccinations,stringency_index,population,population_density,median_age,aged_65_older,aged_70_older,gdp_per_capita,extreme_poverty,cardiovasc_death_rate,diabetes_prevalence,female_smokers,male_smokers,handwashing_facilities,hospital_beds_per_thousand,life_expectancy,human_development_index\n",
    "2020-01-22,Albania,0.0,0.0,0.0,0.0,,,,,,,,,,,,,2877797.0,104.6,38.2,14.8,4.8,11843.0,,,,,,,,,78.5,0.796\n",
    "2020-01-22,Andorra,0.0,0.0,0.0,0.0,,,,,,,,,,,,,77265.0,,,,,,,,,,,,,,,\n",
    "2020-01-22,Angola,0.0,0.0,0.0,0.0,,,,,,,,,,,,,32866272.0,26.3,16.7,2.5,0.7,6779.0,,,,,,,,,60.8,0.581\n",
    "\"\"\"\n",
    "google_cloud_csv = \"\"\"date,location_key,new_confirmed,new_deceased,new_recovered,new_tested,total_confirmed,total_deceased,total_recovered,total_tested,population,population_density,aged_65_older,gdp_per_capita,smoking_prevalence\n",
    "2020-01-22,AFG,0.0,0.0,,,0.0,0.0,,,,38928341.0,58.0,3.0,1803.0,7.1\n",
    "2020-01-22,AGO,0.0,0.0,,,0.0,0.0,,,,32866272.0,26.0,2.5,6779.0,7.9\n",
    "2020-01-22,ALB,0.0,0.0,,,0.0,0.0,,,,2877797.0,104.0,14.8,11843.0,\n",
    "\"\"\"\n",
    "csse_confirmed_csv = \"\"\"Province/State,Country/Region,Lat,Long,1/22/20\n",
    "NaN,Thailand,15.0000,101.0000,2\n",
    "NaN,Japan,36.0000,138.0000,2\n",
    "\"\"\"\n",
    "csse_deaths_csv = \"\"\"Province/State,Country/Region,Lat,Long,1/22/20\n",
    "NaN,Thailand,15.0000,101.0000,0\n",
    "NaN,Japan,36.0000,138.0000,0\n",
    "\"\"\"\n",
    "who_csv = \"\"\"Date_reported,Country,New_cases,New_deaths,Cumulative_cases,Cumulative_deaths\n",
    "2020-01-22,Thailand,0,0,3,0\n",
    "2020-01-22,Japan,0,0,2,0\n",
    "\"\"\"\n",
    "\n",
    "# Read sample data into DataFrames\n",
    "owid_data = pd.read_csv(StringIO(owid_csv))\n",
    "google_cloud_data = pd.read_csv(StringIO(google_cloud_csv))\n",
    "csse_confirmed = pd.read_csv(StringIO(csse_confirmed_csv))\n",
    "csse_deaths = pd.read_csv(StringIO(csse_deaths_csv))\n",
    "who_data = pd.read_csv(StringIO(who_csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5893e48d-c153-43d8-b192-6f714865817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean OWID data\n",
    "def clean_owid(data):\n",
    "    data = data.rename(columns={\"date\": \"Date\", \"location\": \"Country\", \"new_cases\": \"NewCases\", \"new_deaths\": \"NewDeaths\"})\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    return data\n",
    "\n",
    "# Function to clean Google Cloud data\n",
    "def clean_google_cloud(data):\n",
    "    data = data.rename(columns={\"date\": \"Date\", \"location_key\": \"Country\", \"new_confirmed\": \"NewCases\", \"new_deceased\": \"NewDeaths\"})\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors='coerce')\n",
    "    data = data.dropna(subset=[\"Date\"])  # Remove rows with invalid dates\n",
    "    return data\n",
    "\n",
    "# Function to clean CSSE Confirmed Cases data\n",
    "def clean_csse_confirmed(data):\n",
    "    data = data.melt(id_vars=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], var_name=\"Date\", value_name=\"ConfirmedCases\")\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors='coerce')\n",
    "    data = data.dropna(subset=[\"Date\"])  # Remove rows with invalid dates\n",
    "    return data\n",
    "\n",
    "# Function to clean CSSE Deaths data\n",
    "def clean_csse_deaths(data):\n",
    "    data = data.melt(id_vars=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], var_name=\"Date\", value_name=\"Deaths\")\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors='coerce')\n",
    "    data = data.dropna(subset=[\"Date\"])  # Remove rows with invalid dates\n",
    "    return data\n",
    "\n",
    "# Function to clean WHO data\n",
    "def clean_who(data):\n",
    "    data = data.rename(columns={\"Date_reported\": \"Date\", \"Country\": \"Country\", \"New_cases\": \"NewCases\", \"New_deaths\": \"NewDeaths\"})\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    return data\n",
    "\n",
    "# Clean datasets\n",
    "owid_data_cleaned = clean_owid(owid_data)\n",
    "google_cloud_data_cleaned = clean_google_cloud(google_cloud_data)\n",
    "csse_confirmed_cleaned = clean_csse_confirmed(csse_confirmed)\n",
    "csse_deaths_cleaned = clean_csse_deaths(csse_deaths)\n",
    "who_data_cleaned = clean_who(who_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbdc057-0989-4f95-8015-1bd208b8cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 15:11:24,218 - INFO - Merged Data:\n",
      "2024-08-03 15:11:24,219 - INFO -         Date   Country  NewCases_owid  NewDeaths_owid  total_cases  \\\n",
      "0 2020-01-22   Albania            0.0             0.0          0.0   \n",
      "1 2020-01-22   Andorra            0.0             0.0          0.0   \n",
      "2 2020-01-22    Angola            0.0             0.0          0.0   \n",
      "3 2020-01-22  Thailand            0.0             0.0          0.0   \n",
      "4 2020-01-22     Japan            0.0             0.0          0.0   \n",
      "\n",
      "   total_deaths  reproduction_rate  icu_patients  hosp_patients  \\\n",
      "0           0.0                0.0           0.0            0.0   \n",
      "1           0.0                0.0           0.0            0.0   \n",
      "2           0.0                0.0           0.0            0.0   \n",
      "3           0.0                0.0           0.0            0.0   \n",
      "4           0.0                0.0           0.0            0.0   \n",
      "\n",
      "   weekly_icu_admissions  ...  Province/State   Country   Lat   Long  \\\n",
      "0                    0.0  ...             0.0       NaN   0.0    0.0   \n",
      "1                    0.0  ...             0.0       NaN   0.0    0.0   \n",
      "2                    0.0  ...             0.0       NaN   0.0    0.0   \n",
      "3                    0.0  ...             0.0  Thailand  15.0  101.0   \n",
      "4                    0.0  ...             0.0     Japan  36.0  138.0   \n",
      "\n",
      "   ConfirmedCases  Province/State_csse_deaths  Country/Region_csse_deaths  \\\n",
      "0             0.0                         0.0                         NaN   \n",
      "1             0.0                         0.0                         NaN   \n",
      "2             0.0                         0.0                         NaN   \n",
      "3             2.0                         0.0                    Thailand   \n",
      "4             2.0                         0.0                       Japan   \n",
      "\n",
      "   Lat_csse_deaths  Long_csse_deaths  Deaths  \n",
      "0              0.0               0.0     0.0  \n",
      "1              0.0               0.0     0.0  \n",
      "2              0.0               0.0     0.0  \n",
      "3             15.0             101.0     0.0  \n",
      "4             36.0             138.0     0.0  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets on Country and Date\n",
    "merged_data = pd.merge(owid_data_cleaned, google_cloud_data_cleaned, on=[\"Country\", \"Date\"], suffixes=(\"_owid\", \"_gc\"), how=\"outer\")\n",
    "merged_data = pd.merge(merged_data, who_data_cleaned, on=[\"Country\", \"Date\"], suffixes=(\"\", \"_who\"), how=\"outer\")\n",
    "merged_data = pd.merge(merged_data, csse_confirmed_cleaned, left_on=[\"Country\", \"Date\"], right_on=[\"Country/Region\", \"Date\"], suffixes=(\"\", \"_csse\"), how=\"outer\")\n",
    "merged_data = pd.merge(merged_data, csse_deaths_cleaned, left_on=[\"Country\", \"Date\"], right_on=[\"Country/Region\", \"Date\"], suffixes=(\"\", \"_csse_deaths\"), how=\"outer\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "merged_data = merged_data.rename(columns={\"Country/Region\": \"Country\"})\n",
    "\n",
    "# Fill NaN values with 0 for numerical columns where appropriate\n",
    "for col in merged_data.select_dtypes(include=[np.number]).columns:\n",
    "    merged_data[col] = merged_data[col].fillna(0)\n",
    "\n",
    "# Inspect merged data\n",
    "logging.info(\"Merged Data:\")\n",
    "logging.info(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11bf6543-f120-477b-b359-3c4fb190b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 15:11:25,266 - INFO - Final dataset saved to final_covid19_data_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the final merged dataset\n",
    "final_dataset_path = \"final_covid19_data_sample.csv\"\n",
    "merged_data.to_csv(final_dataset_path, index=False)\n",
    "logging.info(f\"Final dataset saved to {final_dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f50f-ed51-4442-b392-38d4ab26ef0d",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2344b-9f48-40a0-9482-ef1218af38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "# Function to read sample data from a CSV string\n",
    "def read_sample_data(csv_string):\n",
    "    return pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "# Function to parse and verify date columns\n",
    "def parse_date(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    if df[date_column].isnull().any():\n",
    "        print(f\"Warning: Some dates in {date_column} could not be parsed. Please check the data.\")\n",
    "\n",
    "# Function to check and convert numeric columns\n",
    "def check_numeric(df, numeric_columns):\n",
    "    for col in numeric_columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            if df[col].isnull().any():\n",
    "                print(f\"Warning: Some values in {col} could not be converted to numeric. Please check the data.\")\n",
    "\n",
    "# Function to handle missing values\n",
    "def handle_missing_values(df):\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "# Function to merge datasets\n",
    "def merge_datasets(datasets, keys):\n",
    "    merged_df = datasets[0]\n",
    "    for i in range(1, len(datasets)):\n",
    "        merged_df = pd.merge(merged_df, datasets[i], on=keys, how='outer')\n",
    "    return merged_df\n",
    "\n",
    "# Sample data strings\n",
    "owid_csv = \"\"\"Date,Country,NewCases_owid,NewDeaths_owid,total_cases,total_deaths,reproduction_rate,icu_patients,hosp_patients,weekly_icu_admissions,weekly_hosp_admissions,total_tests,new_tests,total_vaccinations,people_vaccinated,people_fully_vaccinated,total_boosters,new_vaccinations,stringency_index,population_owid,population_density_owid,median_age,aged_65_older_owid,aged_70_older,gdp_per_capita_owid,extreme_poverty,cardiovasc_death_rate,diabetes_prevalence,female_smokers,male_smokers,handwashing_facilities,hospital_beds_per_thousand,life_expectancy,human_development_index\n",
    "2020-01-22,Albania,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2877797.0,104.6,38.2,14.8,4.8,11843.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,78.5,0.796\n",
    "2020-01-22,Andorra,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,77265.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
    "2020-01-22,Angola,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,32866272.0,26.3,16.7,2.5,0.7,6779.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,60.8,0.581\n",
    "\"\"\"\n",
    "google_cloud_csv = \"\"\"date,location_key,new_confirmed,new_deceased,new_recovered,new_tested,total_confirmed,total_deceased,total_recovered,total_tested,population_gc,population_density_gc,aged_65_older_gc,gdp_per_capita_gc,smoking_prevalence\n",
    "2020-01-22,AL,0,0,0,0,0,0,0,0,2877797,104.6,14.8,11843,0.0\n",
    "2020-01-22,AD,0,0,0,0,0,0,0,0,77265,0.0,0.0,0,0.0\n",
    "2020-01-22,AO,0,0,0,0,0,0,0,0,32866272,26.3,2.5,6779,0.0\n",
    "\"\"\"\n",
    "csse_confirmed_csv = \"\"\"Date,Country/Region,Province/State,Lat,Long,ConfirmedCases\n",
    "2020-01-22,Albania,,41.1533,20.1683,0\n",
    "2020-01-22,Andorra,,42.5063,1.5218,0\n",
    "2020-01-22,Angola,,11.2027,17.8739,0\n",
    "\"\"\"\n",
    "csse_deaths_csv = \"\"\"Date,Country/Region,Province/State,Lat,Long,Deaths\n",
    "2020-01-22,Albania,,41.1533,20.1683,0\n",
    "2020-01-22,Andorra,,42.5063,1.5218,0\n",
    "2020-01-22,Angola,,11.2027,17.8739,0\n",
    "\"\"\"\n",
    "who_csv = \"\"\"Date,Country,NewCases,NewDeaths,Cumulative_cases,Cumulative_deaths\n",
    "2020-01-22,Albania,0,0,0,0\n",
    "2020-01-22,Andorra,0,0,0,0\n",
    "2020-01-22,Angola,0,0,0,0\n",
    "\"\"\"\n",
    "\n",
    "# Read sample data\n",
    "owid_data = read_sample_data(owid_csv)\n",
    "google_cloud_data = read_sample_data(google_cloud_csv)\n",
    "csse_confirmed = read_sample_data(csse_confirmed_csv)\n",
    "csse_deaths = read_sample_data(csse_deaths_csv)\n",
    "who_data = read_sample_data(who_csv)\n",
    "\n",
    "# Parse date columns\n",
    "parse_date(owid_data, \"Date\")\n",
    "parse_date(google_cloud_data, \"date\")\n",
    "parse_date(csse_confirmed, \"Date\")\n",
    "parse_date(csse_deaths, \"Date\")\n",
    "parse_date(who_data, \"Date\")\n",
    "\n",
    "# Check and convert numeric columns\n",
    "owid_numeric_columns = [\"NewCases_owid\", \"NewDeaths_owid\", \"total_cases\", \"total_deaths\", \"reproduction_rate\",\n",
    "                        \"icu_patients\", \"hosp_patients\", \"weekly_icu_admissions\", \"weekly_hosp_admissions\",\n",
    "                        \"total_tests\", \"new_tests\", \"total_vaccinations\", \"people_vaccinated\",\n",
    "                        \"people_fully_vaccinated\", \"total_boosters\", \"new_vaccinations\", \"stringency_index\",\n",
    "                        \"population_owid\", \"population_density_owid\", \"median_age\", \"aged_65_older_owid\",\n",
    "                        \"aged_70_older\", \"gdp_per_capita_owid\", \"extreme_poverty\", \"cardiovasc_death_rate\",\n",
    "                        \"diabetes_prevalence\", \"female_smokers\", \"male_smokers\", \"handwashing_facilities\",\n",
    "                        \"hospital_beds_per_thousand\", \"life_expectancy\", \"human_development_index\"]\n",
    "\n",
    "google_cloud_numeric_columns = [\"new_confirmed\", \"new_deceased\", \"new_recovered\", \"new_tested\", \"total_confirmed\",\n",
    "                                \"total_deceased\", \"total_recovered\", \"total_tested\", \"population_gc\",\n",
    "                                \"population_density_gc\", \"aged_65_older_gc\", \"gdp_per_capita_gc\", \"smoking_prevalence\"]\n",
    "\n",
    "csse_confirmed_numeric_columns = [\"ConfirmedCases\"]\n",
    "csse_deaths_numeric_columns = [\"Deaths\"]\n",
    "who_numeric_columns = [\"NewCases\", \"NewDeaths\", \"Cumulative_cases\", \"Cumulative_deaths\"]\n",
    "\n",
    "check_numeric(owid_data, owid_numeric_columns)\n",
    "check_numeric(google_cloud_data, google_cloud_numeric_columns)\n",
    "check_numeric(csse_confirmed, csse_confirmed_numeric_columns)\n",
    "check_numeric(csse_deaths, csse_deaths_numeric_columns)\n",
    "check_numeric(who_data, who_numeric_columns)\n",
    "\n",
    "# Handle missing values\n",
    "handle_missing_values(owid_data)\n",
    "handle_missing_values(google_cloud_data)\n",
    "handle_missing_values(csse_confirmed)\n",
    "handle_missing_values(csse_deaths)\n",
    "handle_missing_values(who_data)\n",
    "\n",
    "# Merge datasets\n",
    "datasets = [owid_data, google_cloud_data, csse_confirmed, csse_deaths, who_data]\n",
    "keys = [\"Date\", \"Country\"]\n",
    "final_dataset = merge_datasets(datasets, keys)\n",
    "\n",
    "# Review and clean up final dataset\n",
    "# Here you can remove any redundant columns or columns that are not useful\n",
    "\n",
    "# Display the final dataset (For actual output, just remove this line)\n",
    "final_dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
