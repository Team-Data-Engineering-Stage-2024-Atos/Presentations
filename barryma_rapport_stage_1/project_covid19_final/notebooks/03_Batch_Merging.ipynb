{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e3d501-4c92-4dd5-b061-91c01f4e6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 09:55:45,789 - INFO - Merging datasets in batches.\n",
      "2024-08-04 09:55:45,791 - INFO - Merging dataset: owid\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edc2bccba754a7eaa83865fd5684bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging owid: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 09:55:45,839 - INFO - Loading cleaned data in chunks from datasets/cleaned/owid_data_cleaned.csv\n",
      "2024-08-04 09:55:46,037 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:46,334 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:46,834 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: FutureWarning: Passing 'suffixes' which cause duplicate columns {'total_deaths_x', 'new_deaths_x', 'new_cases_x', 'weekly_deaths_x', 'biweekly_cases_x', 'biweekly_deaths_x', 'total_cases_x', 'weekly_cases_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "2024-08-04 09:55:47,626 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:48,778 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:50,491 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:52,763 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:55,554 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:55:58,961 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:03,148 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:08,016 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:13,612 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:20,636 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:28,343 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:37,502 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:47,633 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:56:59,213 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:57:12,373 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:57:26,506 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:57:52,362 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:58:14,038 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:58:35,022 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:58:57,398 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:59:21,923 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:59:47,385 - INFO - Saved merged data to datasets/merged_final_dataset.csv\n",
      "2024-08-04 09:59:47,393 - INFO - Merging dataset: csse_confirmed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4e78a92c6d4a00ab851153b98b26c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging csse_confirmed: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 09:59:47,455 - INFO - Loading cleaned data in chunks from datasets/cleaned/csse_confirmed_data_cleaned.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/307310261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"datasets/merged_final_dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_datasets_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_dataset_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished merging datasets in batches.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70/307310261.py\u001b[0m in \u001b[0;36mmerge_datasets_in_batches\u001b[0;34m(dataset_files, keys, output_file, chunksize)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmerged_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved merged data to {output_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to standardize column names\n",
    "def standardize_columns(df, country_columns):\n",
    "    for col in country_columns:\n",
    "        if col in df.columns:\n",
    "            df.rename(columns={col: 'Country'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# Load cleaned datasets in chunks\n",
    "def load_cleaned_data_in_chunks(file_path, chunksize=10000):\n",
    "    logging.info(f\"Loading cleaned data in chunks from {file_path}\")\n",
    "    country_columns = [\"Country\", \"Country/Region\", \"location\"]\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        standardize_columns(chunk, country_columns)\n",
    "        yield chunk\n",
    "\n",
    "# Merge datasets in chunks\n",
    "def merge_datasets_in_batches(dataset_files, keys, output_file, chunksize=10000):\n",
    "    merged_data = None\n",
    "    for dataset, file_path in dataset_files.items():\n",
    "        logging.info(f\"Merging dataset: {dataset}\")\n",
    "        chunk_iter = load_cleaned_data_in_chunks(file_path, chunksize)\n",
    "        for chunk in tqdm(chunk_iter, desc=f\"Merging {dataset}\"):\n",
    "            if merged_data is None:\n",
    "                merged_data = chunk\n",
    "            else:\n",
    "                merged_data = pd.merge(merged_data, chunk, on=keys, how='outer')\n",
    "            merged_data.to_csv(output_file, index=False)\n",
    "            logging.info(f\"Saved merged data to {output_file}\")\n",
    "    return merged_data\n",
    "\n",
    "# Cleaned datasets paths\n",
    "cleaned_dataset_files = {\n",
    "    \"owid\": \"datasets/cleaned/owid_data_cleaned.csv\",\n",
    "    #\"google_cloud\": \"datasets/cleaned/google_cloud_data_cleaned.csv\",\n",
    "    \"csse_confirmed\": \"datasets/cleaned/csse_confirmed_data_cleaned.csv\",\n",
    "    \"csse_deaths\": \"datasets/cleaned/csse_deaths_data_cleaned.csv\",\n",
    "    \"who\": \"datasets/cleaned/who_data_cleaned.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "logging.info(\"Merging datasets in batches.\")\n",
    "keys = [\"date\", \"Country\"]\n",
    "output_file = \"datasets/merged_final_dataset.csv\"\n",
    "merged_data = merge_datasets_in_batches(cleaned_dataset_files, keys, output_file)\n",
    "logging.info(\"Finished merging datasets in batches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bb4b1-a345-43dc-8b4b-8d22fa2883f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
