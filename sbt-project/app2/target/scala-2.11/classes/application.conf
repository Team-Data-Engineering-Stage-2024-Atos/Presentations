db {
  host = "postgres"
  port = 5432
  database = "sparkdb"
  username = "sparkuser"
  password = "passer"
}

spark {
  appName = "app2"
  master = "yarn"
}

etl {
  input {
    path = "hdfs://master-namenode:9000/data/raw/"
    format = "csv"
  }
  tables {
    dim_customers {
      columns = ["customer_id", "first_name", "last_name", "email", "gender", "date_of_birth", "city", "state", "country"]
      transformations = [
        { column = "email", operation = "lowercase" },
        { column = "first_name", operation = "uppercase" }
      ]
    }
    dim_products {
      columns = ["product_id", "product_name", "category", "subcategory", "brand", "price"]
      transformations = []
    }
    fact_sales {
      columns = ["sale_id", "date_id", "customer_id", "product_id", "quantity", "total_amount"]
      transformations = []
    }
    dim_dates {
      columns = ["date_id", "date", "day_of_week", "month", "quarter", "year"]
      transformations = []
    }
  }
  output {
    target_table = ""
  }
}
