FROM redhat/ubi8:8.10

ARG HADOOP_VERSION=3.3.6
ARG SPARK_VERSION=2.4.7
ARG SCALA_VERSION=2.11.8
ARG SBT_VERSION=1.8.3

# Dependencies
RUN yum install -y java-1.8.0-openjdk-devel wget sudo \
    procps-ng hostname openssh-server openssh-clients sshpass && \
    yum clean all

# Tools
RUN wget https://downloads.lightbend.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz && \
    tar -xvzf scala-${SCALA_VERSION}.tgz && \
    mv scala-${SCALA_VERSION} /usr/local/scala && \
    ln -s /usr/local/scala/bin/scala /usr/local/bin/scala && \
    ln -s /usr/local/scala/bin/scalac /usr/local/bin/scalac && \
    rm scala-${SCALA_VERSION}.tgz

RUN wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xvzf hadoop-${HADOOP_VERSION}.tar.gz && \
    mv hadoop-${HADOOP_VERSION} /usr/local/hadoop && \
    rm hadoop-${HADOOP_VERSION}.tar.gz

RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz && \
    tar -xvzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop2.7 /usr/local/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz

RUN wget https://github.com/sbt/sbt/releases/download/v${SBT_VERSION}/sbt-${SBT_VERSION}.tgz && \
    tar -xvzf sbt-${SBT_VERSION}.tgz && \
    mv sbt /usr/local/sbt && \
    rm sbt-${SBT_VERSION}.tgz


RUN yum install unzip -y && yum clean all

RUN wget https://dlcdn.apache.org/nifi/1.27.0/nifi-1.27.0-bin.zip && \
    unzip nifi-1.27.0-bin.zip && \
    mv nifi-1.27.0 /usr/local/nifi-1.27.0 && \
    rm nifi-1.27.0-bin.zip


RUN wget https://dlcdn.apache.org/nifi/1.27.0/nifi-toolkit-1.27.0-bin.zip && \
    unzip nifi-toolkit-1.27.0-bin.zip  && \
    mv nifi-toolkit-1.27.0 /usr/local/nifi-toolkit-1.27.0 && \
    rm nifi-toolkit-1.27.0-bin.zip

# Environment
ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk \
    HADOOP_HOME=/usr/local/hadoop \
    SPARK_HOME=/usr/local/spark \
    SBT_HOME=/usr/local/sbt \
    SCALA_HOME=/usr/local/scala \
    HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop \
    NIFI_HOME=/usr/local/nifi-1.27.0 \
    #YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop \
    PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$SBT_HOME/bin:$SCALA_HOME/bin:$JAVA_HOME/bin:$NIFI_HOME/bin

RUN wget https://jdbc.postgresql.org/download/postgresql-42.3.1.jar -O $SPARK_HOME/jars/postgresql-42.3.1.jar 
RUN wget https://repo1.maven.org/maven2/org/apache/nifi/nifi-spark-receiver/1.27.0/nifi-spark-receiver-1.27.0.jar -O $SPARK_HOME/jars/nifi-spark-receiver-1.27.0.jar
RUN wget https://repo1.maven.org/maven2/org/apache/nifi/nifi-site-to-site-client/1.27.0/nifi-site-to-site-client-1.27.0.jar -O $SPARK_HOME/jars/nifi-site-to-site-client-1.27.0.jar

RUN useradd -ms /bin/bash hadoopuser && \
    echo "hadoopuser:passer" | chpasswd && usermod -aG wheel hadoopuser && \
    chown -R hadoopuser:hadoopuser /usr/local/hadoop /usr/local/spark && \
    sed -i 's/^# %wheel/%wheel/' /etc/sudoers

RUN echo 'hadoopuser ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers

ENV HDFS_NAMENODE_USER=hadoopuser \
    HDFS_DATANODE_USER=hadoopuser \
    HDFS_SECONDARYNAMENODE_USER=hadoopuser

# SSH configuration
RUN mkdir /var/run/sshd && mkdir -p /home/hadoopuser/.ssh/ && \
    ssh-keygen -t rsa -b 2048 -f /etc/ssh/ssh_host_rsa_key -N '' && \
    ssh-keygen -t ecdsa -b 521 -f /etc/ssh/ssh_host_ecdsa_key -N '' && \
    ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -N '' && \
    ssh-keygen -t rsa -N "" -f /home/hadoopuser/.ssh/id_rsa && \
    cat /home/hadoopuser/.ssh/id_rsa.pub >> /home/hadoopuser/.ssh/authorized_keys && \
    chmod 600 /home/hadoopuser/.ssh/authorized_keys && \
    chown -R hadoopuser:hadoopuser /home/hadoopuser/.ssh

# Ensure the sshd service runs
RUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config

RUN echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk" >> /home/hadoopuser/.bashrc && \
    echo "export HADOOP_HOME=/usr/local/hadoop" >> /home/hadoopuser/.bashrc && \
    echo "export SPARK_HOME=/usr/local/spark" >> /home/hadoopuser/.bashrc && \
    echo "export SBT_HOME=/usr/local/sbt" >> /home/hadoopuser/.bashrc && \
    echo "export SCALA_HOME=/usr/local/scala" >> /home/hadoopuser/.bashrc && \
    echo "export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop" >> /home/hadoopuser/.bashrc && \
    echo "export NIFI_HOME=/usr/local/nifi-1.27.0/" >> /home/hadoopuser/.bashrc && \
    #echo "export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop" >> /home/hadoopuser/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$SBT_HOME/bin:$SCALA_HOME/bin:$JAVA_HOME/bin:$NIFI_HOME/bin' >> /home/hadoopuser/.bashrc

# Finalization

WORKDIR /app

RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk' >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh
COPY hadoop/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
COPY hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
COPY hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml
COPY hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
COPY hadoop/log4j.properties $HADOOP_HOME/etc/hadoop/log4j.properties

COPY spark/spark-defaults.conf $SPARK_HOME/conf/spark-defaults.conf
COPY spark/spark-env.sh $SPARK_HOME/conf/spark-env.sh
COPY spark/log4j.properties $SPARK_HOME/conf/log4j.properties

COPY nifi/conf/* $NIFI_HOME/conf/

COPY entrypoint-master.sh /usr/local/bin/entrypoint.sh
RUN chmod +x /usr/local/bin/entrypoint.sh

EXPOSE 22 9870 8088 7077 8080 18080 8090

USER root
RUN chmod 600 /etc/ssh/sshd_config && chown -R hadoopuser:hadoopuser /usr/local/nifi-1.27.0/ && chmod -R 755 /usr/local/nifi-1.27.0/


USER hadoopuser

CMD [ "/usr/local/bin/entrypoint.sh" ]
